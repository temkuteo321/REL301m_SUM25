# 1.Káº¿ thá»«a tá»« Temporal-Difference vÃ  GPI:
- Sarsa lÃ  thuáº­t toÃ¡n Ä‘iá»u khiá»ƒn trÃªn-policy dá»±a trÃªn TD (Temporal Difference), thá»±c hiá»‡n Generalized Policy Iteration (GPI): láº·p láº¡i giá»¯a viá»‡c Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ hÃ nh Ä‘á»™ng (policy evaluation) vÃ  cáº£i thiá»‡n chÃ­nh sÃ¡ch (policy improvement)
- Cáº­p nháº­t giÃ¡ trá»‹ sau má»—i bÆ°á»›c (bootstrapping):
  
![image](https://github.com/user-attachments/assets/941c0e20-c76e-43c2-9c3f-d6cfe5b027ed)


trong Ä‘Ã³ 
ğ´
â€²
A 
â€²
  lÃ  hÃ nh Ä‘á»™ng Ä‘Æ°á»£c chá»n theo chÃ­nh sÃ¡ch Îµâ€‘greedy
# 2.CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng:
- Báº¯t Ä‘áº§u tá»« tráº¡ng thÃ¡i S, chá»n hÃ nh Ä‘á»™ng A theo Îµ-greedy, thá»±c hiá»‡n, nháº­n pháº§n thÆ°á»Ÿng R vÃ  tráº¡ng thÃ¡i S'.

- Láº­p tá»©c chá»n hÃ nh Ä‘á»™ng káº¿ tiáº¿p A' theo Îµ-greedy (on-policy), rá»“i cáº­p nháº­t Q(S,A).

- QuÃ¡ trÃ¬nh tiáº¿p tá»¥c cho Ä‘áº¿n khi káº¿t thÃºc (terminal)
# 3. Æ¯u tháº¿:
- On-policy: luÃ´n há»c vÃ  thá»±c thi theo cÃ¹ng chÃ­nh sÃ¡ch, giÃºp mÃ´ hÃ¬nh pháº£n á»©ng chÃ­nh xÃ¡c vá»›i cÃ¡ch hÃ nh xá»­ thá»±c táº¿ .
- Bootstrapping & cáº­p nháº­t tá»©c thá»i: giÃºp Sarsa há»c trá»±c tuyáº¿n nhanh chÃ³ng sau má»—i bÆ°á»›c, phÃ¹ há»£p vá»›i cÃ¡c task dÃ i hoáº·c khÃ´ng káº¿t thÃºc rÃµ rÃ ng 
- Hiá»‡u quáº£ tá»« GPI: káº¿t há»£p lá»£i Ã­ch cá»§a TD vÃ  cáº£i thiá»‡n chÃ­nh sÃ¡ch liÃªn tá»¥c giÃºp há»™i tá»¥ nhanh hÆ¡n so vá»›i Monte Carlo thuáº§n tÃºy
# 4. So sÃ¡nh vá»›i Qâ€‘learning & Expected Sarsa:
- Qâ€‘learning (off-policy): cáº­p nháº­t nháº¯m vÃ o giÃ¡ trá»‹ tá»‘i Ä‘a 
max
â¡
ğ‘
ğ‘„
(
ğ‘†
â€²
,
ğ‘
)
max 
a
â€‹
 Q(S 
â€²
 ,a), khÃ´ng phá»¥ thuá»™c vÃ o hÃ nh vi thá»±c thi. VÃ¬ váº­y, sai sá»‘ Ä‘á»‹nh kiáº¿n (bias) cÃ³ thá»ƒ tÄƒng do â€œmaximization biasâ€
- Expected Sarsa: sá»­ dá»¥ng ká»³ vá»ng theo phÃ¢n phá»‘i Îµâ€‘greedy thay vÃ¬ giÃ¡ trá»‹ thá»±c nghiá»‡m cá»§a A', giÃºp giáº£m phÆ°Æ¡ng sai cáº­p nháº­t, nhÆ°ng tá»‘n phÃ©p tÃ­nh hÆ¡n
