
# Mục tiêu của Policy Gradient:
![image](https://github.com/user-attachments/assets/872b0a3c-45fd-465f-90fb-ced08781c755)
# Công thức Policy Gradient Theorem:
![image](https://github.com/user-attachments/assets/d07789a5-9531-43b3-847f-8ea63b3e6c96)
# REINFORCE - Policy Gradient đơn giản (Monte Carlo):
![image](https://github.com/user-attachments/assets/379defed-aaa4-4528-9ca4-62fd22ea3964)
# Giảm độ biến thiên: Baseline và Advantage:
![image](https://github.com/user-attachments/assets/85bc3961-8d17-420f-a76f-abd16f375a5c)
# Tích hợp với Function Approximation:
![image](https://github.com/user-attachments/assets/b953da23-5d6b-4225-98f3-c1c4416f7700)
# Cải tiến thực tế:
- Trích xuất gradient qua reparameterization cho các policy Gaussian giúp giảm variance
- Các thuật toán như Expected Policy Gradients (EPG), PPO, TRPO được phát triển để nâng cao ổn định và hiệu suất đạo hàm
